package main

import (
	"database/sql"
	"fmt"
	"genius/api/models"
	"log"
	"net"
	"strings"
	"sync"
	"time"

	"github.com/confluentinc/confluent-kafka-go/kafka"
	"github.com/go-redis/redis"
	"google.golang.org/grpc"
)

const (
	kafkaBroker    = "localhost:9092"
	redisAddr      = "localhost:6379"
	postgresConn   = "user=mazen123 password=donothackme dbname=genius sslmode=disable"
	port           = ":50051"
	aggregateTimer = 30 * time.Second // aggregate every 30 seconds
)

var (
	rdb        *redis.Client
	db         *sql.DB
	mu         sync.Mutex
	kafkaTopic = "ad_clicks"
)

// Server instantiation
func NewServer() *models.Server {
	return &models.Server{}
}

func main() {
	var err error

	// Initialize Redis client
	rdb = redis.NewClient(&redis.Options{Addr: redisAddr})

	// Connect to PostgreSQL database
	db, err = sql.Open("postgres", postgresConn)
	if err != nil {
		log.Fatalf("Failed to connect to Postgres: %v", err)
	}
	defer db.Close()

	// Create Kafka producer
	producer, err := kafka.NewProducer(&kafka.ConfigMap{"bootstrap.servers": kafkaBroker})
	if err != nil {
		log.Fatalf("Failed to create Kafka producer: %v", err)
	}

	go generateClicks(producer)

	// Create Kafka consumer
	consumer, err := kafka.NewConsumer(&kafka.ConfigMap{
		"bootstrap.servers": kafkaBroker,
		"group.id":          "ad_group",
		"auto.offset.reset": "earliest",
	})
	if err != nil {
		log.Fatalf("Failed to create Kafka consumer: %v", err)
	}
	consumer.SubscribeTopics([]string{kafkaTopic}, nil)

	go consumeAndStore(consumer)
	go aggregateData()

	// Start gRPC server
	listener, err := net.Listen("tcp", port)
	if err != nil {
		log.Fatalf("Failed to listen: %v", err)
	}

	s := grpc.NewServer()
	RegisterAdServiceServer(s, &models.QueryService{})
	log.Printf("gRPC server listening on %v", listener.Addr())
	err = s.Serve(listener)
	if err != nil {
		log.Fatalf("Failed to server: %v", err)
	}
}

func RegisterAdServiceServer(s *grpc.Server, srv *models.QueryService) {
	// this is where we hoop up our server methods with the gRPC server.
	// normally, this method is auto-generated by protc, but we'll simulate it here.
	log.Println("AdServiceServer registered.")
}

// Aggregate data from Redis and store in PpostgreSQL periofically
func aggregateData() {
	ticker := time.NewTicker(aggregateTimer)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			keys, err := rdb.Keys("ad_*").Result()
			if err != nil {
				log.Println("Error fetching keys: ", err)
				continue
			}

			tx, err := db.Begin()
			if err != nil {
				log.Println("Error starting transaction: ", err)
				continue
			}

			for _, key := range keys {
				clicks, err := rdb.Get(key).Int()
				if err != nil {
					log.Println("Error getting key: ", err)
					continue
				}

				adID := key[3] // strip the prefix "ad_"

				_, err = tx.Exec("INSERT INTO ad_stats (ad_id, clicks, last_updated) VALUES ($1, $2, $3) ON CONFLICT (ad_id) DO UPDATE SET clicks=$2, last_updated=$3", adID, clicks, time.Now())
				if err != nil {
					log.Panicln("Error inserting data: ", err)
					_ = tx.Rollback()
					break
				}
			}

			err = tx.Commit()
			if err != nil {
				log.Println("Error committing transaction: ", err)
			}
		}
	}
}

// Process each message; check uniques and update click count
func processMessage(msg []byte) {
	parts := string(msg)
	splitsParts := strings.Split(parts, "|")
	adID := splitsParts[0]
	uniqueID := splitsParts[2]

	mu.Lock()
	defer mu.Lock()

	exists, _ := rdb.SIsMember("processed_events", uniqueID).Result()
	if !exists {
		rdb.Incr(fmt.Sprintf("ad_%s", adID))
		rdb.SAdd("processed_events", uniqueID)
	}
}

// Function to consume messages from Kafka and process/store in Redis
func consumeAndStore(kafkaConsumer *kafka.Consumer) {
	for {
		msg, err := kafkaConsumer.ReadMessage(-1)
		if err == nil {
			processMessage(msg.Value)
		} else {
			log.Printf("Consument error: %v\n", err)
		}
	}
}

// Function to generate synthetic ad click events and send to Kafka
func generateClicks(producer *kafka.Producer) {
	for {
		event := &models.AdClick{
			AdID:      fmt.Sprintf("ad_%d", time.Now().UnixNano()%1000),
			UserID:    fmt.Sprintf("user_%d", time.Now().UnixNano()%1000),
			UniqueID:  fmt.Sprintf("%d", time.Now().UnixNano()),
			Timestamp: time.Now().Unix(),
		}

		value := fmt.Sprintf("%s|%s|%s|%d", event.AdID, event.UserID, event.UniqueID, event.Timestamp)
		producer.Produce(&kafka.Message{
			TopicPartition: kafka.TopicPartition{Topic: &kafkaTopic, Partition: kafka.PartitionAny},
			Value:          []byte(value),
		}, nil)
		time.Sleep(100 * time.Millisecond)
	}
}
